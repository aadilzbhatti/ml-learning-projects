{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1c6a0c-6520-454d-af1d-d8d8b32394ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5603b80d-f897-4312-a473-db00d87d7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8cc13c7-92c2-415a-91a7-b841387986a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('makemore/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f9d90e-8b2e-495a-b3c5-1d4ab0c9efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words, block_size):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "    \n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf097d0-b6c8-4360-a5aa-61574e6526b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
    "# Xte, Yte = build_dataset(words[n2:], block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2612c4b6-775b-4800-9017-18d577924643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 17:09:16,157] A new study created in memory with name: no-name-2fe80b2c-9889-482b-867d-52ffde24e1a5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 10])torch.Size([182625, 9]) torch.Size([182625])\n",
      "torch.Size([182625, 13]) torch.Size([182625])\n",
      " torch.Size([182625])\n",
      "torch.Size([182625, 15]) torch.Size([182625])\n",
      "torch.Size([182625, 13]) torch.Size([182625])\n",
      "torch.Size([182625, 10]) torch.Size([182625])\n",
      "torch.Size([182625, 7]) torch.Size([182625])\n",
      "torch.Size([182625, 7]) torch.Size([182625])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py:40: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  if not param.grad:\n",
      "/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param.data -= learning_rate * param.grad\n",
      "[W 2025-03-01 17:09:18,562] Trial 3 failed with parameters: {'block_size': 9, 'embedding_dims': 10, 'l2_num_neurons': 140, 'batch_size': 16, 'learning_rate': 0.047871674439245546} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,563] Trial 2 failed with parameters: {'block_size': 13, 'embedding_dims': 19, 'l2_num_neurons': 111, 'batch_size': 64, 'learning_rate': 0.008445331755882274} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,563] Trial 4 failed with parameters: {'block_size': 15, 'embedding_dims': 19, 'l2_num_neurons': 56, 'batch_size': 16, 'learning_rate': 0.08705996514269923} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,563] Trial 1 failed with parameters: {'block_size': 10, 'embedding_dims': 8, 'l2_num_neurons': 94, 'batch_size': 16, 'learning_rate': 0.00613614148829192} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,563] Trial 6 failed with parameters: {'block_size': 13, 'embedding_dims': 13, 'l2_num_neurons': 157, 'batch_size': 16, 'learning_rate': 0.014982700253223354} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,563] Trial 0 failed with parameters: {'block_size': 7, 'embedding_dims': 6, 'l2_num_neurons': 217, 'batch_size': 16, 'learning_rate': 0.0024280047646859336} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,564] Trial 5 failed with parameters: {'block_size': 10, 'embedding_dims': 16, 'l2_num_neurons': 207, 'batch_size': 64, 'learning_rate': 0.00184616265419063} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,564] Trial 7 failed with parameters: {'block_size': 7, 'embedding_dims': 15, 'l2_num_neurons': 233, 'batch_size': 16, 'learning_rate': 0.003163638768409441} because of the following error: TypeError(\"unsupported operand type(s) for *: 'float' and 'NoneType'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 58, in objective\n",
      "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
      "                       learning_rate=learning_rate, block_size=block_size,\n",
      "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
      "  File \"/var/folders/8_/kl72rt_d67z2yxqnyg9_n3mw0000gn/T/ipykernel_26052/122202842.py\", line 42, in train_model\n",
      "    param.data -= learning_rate * param.grad\n",
      "                  ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "[W 2025-03-01 17:09:18,564] Trial 3 failed with value None.\n",
      "[W 2025-03-01 17:09:18,564] Trial 2 failed with value None.\n",
      "[W 2025-03-01 17:09:18,565] Trial 4 failed with value None.\n",
      "[W 2025-03-01 17:09:18,565] Trial 1 failed with value None.\n",
      "[W 2025-03-01 17:09:18,566] Trial 6 failed with value None.\n",
      "[W 2025-03-01 17:09:18,566] Trial 0 failed with value None.\n",
      "[W 2025-03-01 17:09:18,567] Trial 5 failed with value None.\n",
      "[W 2025-03-01 17:09:18,567] Trial 7 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did it fail at:Why did it fail at: 0\n",
      "Why did it fail at: 0\n",
      "Why did it fail at: 0\n",
      " 0\n",
      "Why did it fail at: 0\n",
      "Why did it fail at: 0\n",
      "Why did it fail at: 0\n",
      "Why did it fail at: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Create a study with GPU support and parallel optimization\u001b[39;00m\n\u001b[1;32m     65\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use all available CPU cores, including parallel GPU support\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py:100\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[38;5;66;03m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n\u001b[0;32m--> 100\u001b[0m                         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m                 futures\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    103\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    104\u001b[0m                         _optimize_sequential,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     )\n\u001b[1;32m    116\u001b[0m                 )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[29], line 58\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     56\u001b[0m Xtr, Ytr \u001b[38;5;241m=\u001b[39m build_dataset(words[:n1], block_size)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Assuming Xtr and Ytr are your training data\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                   \u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_num_neurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_num_neurons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[29], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(device, Xtr, Ytr, steps, batch_size, learning_rate, block_size, embedding_dims, l2_num_neurons)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad:\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy did it fail at:\u001b[39m\u001b[38;5;124m\"\u001b[39m, idx)\n\u001b[0;32m---> 42\u001b[0m         param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Define your model as before\n",
    "def create_model(embedding_dims, block_size, l2_num_neurons):\n",
    "    C = torch.randn((27, embedding_dims))\n",
    "    W1 = torch.randn((embedding_dims * block_size, l2_num_neurons))\n",
    "    b1 = torch.randn(l2_num_neurons)\n",
    "    W2 = torch.randn((l2_num_neurons, 27))\n",
    "    b2 = torch.randn(27)\n",
    "    for p in [C, W1, b1, W2, b2]:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return C, W1, b1, W2, b2\n",
    "\n",
    "def train_model(device, Xtr, Ytr, steps, batch_size, learning_rate, block_size, embedding_dims, l2_num_neurons):\n",
    "    C, W1, b1, W2, b2 = create_model(embedding_dims, block_size, l2_num_neurons)\n",
    "    \n",
    "    # Move model to the correct device (MPS if available, otherwise CPU)\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_num_threads(torch.get_num_threads())\n",
    "    C, W1, b1, W2, b2 = C.to(device), W1.to(device), b1.to(device), W2.to(device), b2.to(device)\n",
    "    Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "    \n",
    "    # Training loop as before\n",
    "    for step in range(steps):\n",
    "        # Minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        emb = C[Xtr[ix]]\n",
    "        h = torch.tanh(emb.view(-1, embedding_dims * block_size) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Ytr[ix])\n",
    "        \n",
    "        # Backward pass\n",
    "        for param in [C, W1, b1, W2, b2]:\n",
    "            param.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        for idx, param in enumerate([C, W1, b1, W2, b2]):\n",
    "            if not param.grad:\n",
    "                print(\"Why did it fail at:\", idx)\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def objective(trial):\n",
    "    block_size = trial.suggest_int(\"block_size\", 5, 15)\n",
    "    embedding_dims = trial.suggest_int(\"embedding_dims\", 5, 20)\n",
    "    l2_num_neurons = trial.suggest_int(\"l2_num_neurons\", 50, 300)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True)\n",
    "\n",
    "    # Set the number of training steps\n",
    "    steps = 10000\n",
    "\n",
    "    Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "    # Assuming Xtr and Ytr are your training data\n",
    "    loss = train_model(device='mps', Xtr=Xtr, Ytr=Ytr, steps=steps, batch_size=batch_size,\n",
    "                       learning_rate=learning_rate, block_size=block_size, \n",
    "                       embedding_dims=embedding_dims, l2_num_neurons=l2_num_neurons)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Create a study with GPU support and parallel optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)  # Use all available CPU cores, including parallel GPU support\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ada42a0-71e1-4319-9d0f-8cc9c55f6ded",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
